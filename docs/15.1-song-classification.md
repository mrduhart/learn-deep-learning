# Song classification

- The librosa package is awesome for (audio) signal processing

- The GTZAN dataset is canon for music modeling, but difficult to obtain because of copyrights
  - Kaggle has it + other nice features: https://www.kaggle.com/andradaolteanu/gtzan-dataset-music-genre-classification
  - 1000 songs, 10 genres, 30 seconds recordings (not exactly 30 s)

- A melspectrogram resembles human hearing
  - Not linear (low vs high frequencies)
  - Transformation of the FFT
  - Shape: number of bands (128) x number of frames
  - Visualized with librosa.display.specshow (if you want to understand what the hell is going on)

- 1D CNNs are good for sequences (are they better vs RNNs?)
  - Input layer receives windows with some number of samples = number input units
  - Number of channels = depth of each sample = number of bands in spectrogram

- Modeling: data augmentation, training, evaluation
  - Split each song in 3-second fragments: split_10 (np.reshape)
  - Early stopping actually worked: more epochs overfit a lot + no actual improvement in val_acc
  - Accuracy on 3-second fragments is poor ~= 63%
  - Max-vote over all 3-second fragments per song increases accuracy ~= 75%
    - Voting could also work, author says it didn't
  - Confusion matrices rock for spotting improvements (which classes are harder?)
